{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ceb2ba6",
   "metadata": {},
   "source": [
    "# Darwin Dataset\n",
    "#This notebook will look to calling national rail Darwin API to download and extract Darwin Timetable Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa2531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "import boto3\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc6bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name='eu-west-1',\n",
    "    aws_access_key_id = 'SECRET',\n",
    "    aws_secret_access_key = 'SECRET'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdad666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPTimetable/20220523023948_v8.xml.gz\n",
      "PPTimetable/20220524023843_v8.xml.gz\n",
      "PPTimetable/20220525023831_v8.xml.gz\n",
      "PPTimetable/20220526023801_v8.xml.gz\n",
      "PPTimetable/20220527023502_v8.xml.gz\n",
      "PPTimetable/20220528022757_v8.xml.gz\n",
      "PPTimetable/20220529022938_v8.xml.gz\n",
      "PPTimetable/20220530023632_v8.xml.gz\n"
     ]
    }
   ],
   "source": [
    "#set the bucket\n",
    "\n",
    "#Check if directory exists\n",
    "\n",
    "directory  = \"C:/Users/Mohammed/myFolder/Output\"\n",
    "    \n",
    "#os.mkdir(\"C:/Users/Mohammed/myFolder/Output\")\n",
    "\n",
    "\n",
    "bucket = s3.Bucket('darwin.xmltimetable')\n",
    "#print the objects within the bucket\n",
    "for obj in bucket.objects.all():\n",
    "    if re.search(\"_v8.xml.gz\", obj.key):\n",
    "        temp_string = str(obj.key).split(\"/\")[1]\n",
    "        #print(temp_string)\n",
    "        s3.Bucket('darwin.xmltimetable').download_file(obj.key, Filename = directory + \"/\" + temp_string)\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d90e48eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "temp_array = []\n",
    "filepath = \"C:/Users/Mohammed/myFolder/Output/20220522022909_v8.xml.gz\"\n",
    "\n",
    "def extract_and_append(filepath):\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        file_content = f.read()\n",
    "    \n",
    "    #open filecontent xml\n",
    "    d = xmltodict.parse(file_content)\n",
    "    \n",
    "    #read xml into a dataframe.\n",
    "    #read only the Journey Node from the XML document\n",
    "    df = pd.DataFrame.from_dict(d['PportTimetable']['Journey'])\n",
    "    \n",
    "    #iterate through the dataframe relevant columns and extract the values into a temp array\n",
    "    cols = [\"OR\", \"IP\", \"PP\", \"DT\"]\n",
    "    status = [\"User\", \"User\", \"Passing Point\", \"User\"]\n",
    "\n",
    "    for item in zip(cols, status):\n",
    "        for index, row in df.iterrows():\n",
    "            #print(row[\"@rid\"])\n",
    "            TOC = row[\"@toc\"]\n",
    "            #print(row[\"IP\"])\n",
    "            if type(row[item[0]]) == list:\n",
    "                for i in range(len(row[item[0]])):\n",
    "                    pp_tiploc = row[item[0]][i][\"@tpl\"]\n",
    "                    temp_array.append([TOC, item[1], pp_tiploc])\n",
    "\n",
    "                    \n",
    "                    \n",
    "print(len(temp_array))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79112e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Mohammed/myFolder/Output\\20220522022909_v8.xml.gz\n",
      "C:/Users/Mohammed/myFolder/Output\\20220523023948_v8.xml.gz\n",
      "C:/Users/Mohammed/myFolder/Output\\20220524023843_v8.xml.gz\n",
      "C:/Users/Mohammed/myFolder/Output\\20220525023831_v8.xml.gz\n",
      "C:/Users/Mohammed/myFolder/Output\\20220526023801_v8.xml.gz\n",
      "C:/Users/Mohammed/myFolder/Output\\20220527023502_v8.xml.gz\n",
      "C:/Users/Mohammed/myFolder/Output\\20220528022757_v8.xml.gz\n",
      "C:/Users/Mohammed/myFolder/Output\\20220529022938_v8.xml.gz\n",
      "C:/Users/Mohammed/myFolder/Output\\20220530023632_v8.xml.gz\n",
      "7369127\n"
     ]
    }
   ],
   "source": [
    "#iterate through a directory and call the function to append_temparray\n",
    "\n",
    "import glob\n",
    "#columns to iterate through\n",
    "\n",
    "for filename in glob.iglob(f'{directory}/*.gz'):\n",
    "    print(filename)\n",
    "    extract_and_append(filename)\n",
    "\n",
    "\n",
    "\n",
    "print(len(temp_array))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b109ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOC</th>\n",
       "      <th>TIPLOC</th>\n",
       "      <th>User/Pass</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AW</td>\n",
       "      <td>ABCWM</td>\n",
       "      <td>Passing Point</td>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AW</td>\n",
       "      <td>ABDVY</td>\n",
       "      <td>User</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AW</td>\n",
       "      <td>ABER</td>\n",
       "      <td>User</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AW</td>\n",
       "      <td>ABGLELE</td>\n",
       "      <td>Passing Point</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AW</td>\n",
       "      <td>ABGLELE</td>\n",
       "      <td>User</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10395</th>\n",
       "      <td>ZZ</td>\n",
       "      <td>WYLYAHB</td>\n",
       "      <td>Passing Point</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10396</th>\n",
       "      <td>ZZ</td>\n",
       "      <td>YATE</td>\n",
       "      <td>Passing Point</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10397</th>\n",
       "      <td>ZZ</td>\n",
       "      <td>YORK</td>\n",
       "      <td>Passing Point</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10398</th>\n",
       "      <td>ZZ</td>\n",
       "      <td>YORK</td>\n",
       "      <td>User</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10399</th>\n",
       "      <td>ZZ</td>\n",
       "      <td>YWAYNJN</td>\n",
       "      <td>Passing Point</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10400 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TOC   TIPLOC      User/Pass  count\n",
       "0      AW    ABCWM  Passing Point    924\n",
       "1      AW    ABDVY           User    330\n",
       "2      AW     ABER           User   1613\n",
       "3      AW  ABGLELE  Passing Point    400\n",
       "4      AW  ABGLELE           User    873\n",
       "...    ..      ...            ...    ...\n",
       "10395  ZZ  WYLYAHB  Passing Point     28\n",
       "10396  ZZ     YATE  Passing Point      6\n",
       "10397  ZZ     YORK  Passing Point      2\n",
       "10398  ZZ     YORK           User      2\n",
       "10399  ZZ  YWAYNJN  Passing Point     14\n",
       "\n",
       "[10400 rows x 4 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the temp_array into a dataframe\n",
    "station_tocs_original = pd.DataFrame(temp_array, columns = [\"TOC\",\"User/Pass\", \"TIPLOC\"])\n",
    "\n",
    "#Reduce the list by removing duplicate TOCs from Stations\n",
    "station_tocs_reduced = station_tocs_original.groupby([\"TOC\", \"TIPLOC\",\"User/Pass\"])[\"TOC\"].count().reset_index(name=\"count\")\n",
    "\n",
    "station_tocs_reduced\n",
    "\n",
    "station_tocs_reduced.to_csv(directory + \"/\" + \"station_tocs_timetable_output.csv\")\n",
    "station_tocs_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1993d",
   "metadata": {},
   "source": [
    "## TOC Station Probabilities\n",
    "This section will look to see the probability of a TOC pulling into a station based on the number of times it has attended/departed from a particular station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6a13adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by Station\n",
    "\n",
    "#Look at users of the station only\n",
    "Station_Tocs_Users = station_tocs_reduced[station_tocs_reduced[\"User/Pass\"]==\"User\"]\n",
    "Station_Tocs_Users\n",
    "\n",
    "#Get total TOC visits to each station\n",
    "station_total= Station_Tocs_Users.groupby([\"TIPLOC\"])[\"count\"].sum().reset_index(name = \"Total Visits\")\n",
    "station_total\n",
    "\n",
    "#work out the ratio of visit by overall visit\n",
    "Station_toc_users_total = Station_Tocs_Users.merge(station_total, how=\"left\", on=\"TIPLOC\")\n",
    "#station_probabilities\n",
    "Station_toc_users_total[\"Frequency %\"] = round((Station_toc_users_total[\"count\"] / Station_toc_users_total[\"Total Visits\"])*100,2)\n",
    "Station_toc_users_total[\"TOC_New\"] = Station_toc_users_total[\"TOC\"] + \" (\" + Station_toc_users_total[\"Frequency %\"].astype(str) + \"%)\"\n",
    "\n",
    "#output the file\n",
    "station_toc_probabilities_Final = Station_toc_users_total.groupby('TIPLOC')['TOC_New'].apply('; '.join).reset_index()\n",
    "station_toc_probabilities_Final.to_csv(\"Station_TOC_Probabilities_final_Users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "31772aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at passing point\n",
    "\n",
    "#Look at users of the station only\n",
    "Station_Tocs_PassingPoint = station_tocs_reduced[station_tocs_reduced[\"User/Pass\"]==\"Passing Point\"]\n",
    "station_total_passingPoint= Station_Tocs_PassingPoint.groupby([\"TIPLOC\"])[\"count\"].sum().reset_index(name = \"Total Visits\")\n",
    "station_total_passingPoint\n",
    "\n",
    "#Get total TOC visits to each station\n",
    "Station_toc_users_total_pp = Station_Tocs_PassingPoint.merge(station_total_passingPoint, how=\"left\", on=\"TIPLOC\")\n",
    "Station_toc_users_total_pp\n",
    "\n",
    "#work out the ratio of visit by overall visit\n",
    "#station_probabilities\n",
    "Station_toc_users_total_pp[\"Frequency %\"] = round((Station_toc_users_total_pp[\"count\"] / Station_toc_users_total_pp[\"Total Visits\"])*100,2)\n",
    "Station_toc_users_total_pp[\"TOC_New\"] = Station_toc_users_total_pp[\"TOC\"] + \" (\" + Station_toc_users_total_pp[\"Frequency %\"].astype(str) + \"%)\"\n",
    "\n",
    "#output the file\n",
    "station_toc_probabilities_Final_pp = Station_toc_users_total_pp.groupby('TIPLOC')['TOC_New'].apply('; '.join).reset_index()\n",
    "station_toc_probabilities_Final_pp.to_csv(\"Station_TOC_Probabilities_final_PassingPoint.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f4a9f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional Combine the users and passing points together\n",
    "station_toc_probabilities_Final2 = station_toc_probabilities_Final.merge(station_toc_probabilities_Final_pp, how=\"left\", on=\"TIPLOC\")\n",
    "station_toc_probabilities_Final2.to_csv(\"Station_TOC_Probabilities_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "551df0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebcd897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
